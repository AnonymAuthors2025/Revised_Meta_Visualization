{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "624e727d",
      "metadata": {
        "id": "624e727d"
      },
      "source": [
        "## Meta-Visualization Framework for Spatiotemporal Analytics:\n",
        "### From Data Generation to Advanced Visualization on Maps\n",
        "\n",
        "## **Stage 1: Data Acquisition and Preparation Tools**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c615595",
      "metadata": {
        "id": "5c615595"
      },
      "source": [
        "**Overview**\n",
        "\n",
        "This notebook implements the core data acquisition and preparation methodologies described in our research article, focusing on three key techniques for gathering heterogeneous data sources:\n",
        "\n",
        "1.   Map Scan Processing (Geospatial)\n",
        "2.   Web Scraping (Tabular/Text)\n",
        "3.   IoT Sensor Integration (Real-time)\n",
        "\n",
        "Each section provides executable code examples, best practices, and ethical considerations. Note that the validation and quality of the obtained datasets are discussed in Case_Study repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image(url='https://github.com/AnonymAuthors2025/Revised_Meta_Visualization/blob/main/Stage1/Fig1_pipeline.png?raw=true', width=500)"
      ],
      "metadata": {
        "id": "aMa_XOnQeQdX",
        "outputId": "69c060d8-6f50-4a39-eb99-4397fc64faf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "id": "aMa_XOnQeQdX",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://github.com/AnonymAuthors2025/Revised_Meta_Visualization/blob/main/Stage1/Fig1_pipeline.png?raw=true\" width=\"500\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "1. **Map Scan Processing**\n",
        "\n",
        "Archival maps (PNG/JPEG/PDF) contain valuable geospatial data but are unstructured and require conversion to machine-readable formats (e.g., GeoJSON).\n",
        "\n",
        "\n",
        "To deal with such source of data, we propose a new technique based on segmentation and georeferencing (See next Figure)."
      ],
      "metadata": {
        "id": "nQhrPbtyi9QZ"
      },
      "id": "nQhrPbtyi9QZ"
    },
    {
      "cell_type": "code",
      "source": [
        "Image(url='https://github.com/AnonymAuthors2025/Revised_Meta_Visualization/blob/main/Stage1/Fig2.png?raw=true', width=200)"
      ],
      "metadata": {
        "id": "o7mOegdNjJwH",
        "outputId": "8302c786-440e-4428-902c-6ff85e0ff578",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        }
      },
      "id": "o7mOegdNjJwH",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://github.com/AnonymAuthors2025/Revised_Meta_Visualization/blob/main/Stage1/Fig2.png?raw=true\" width=\"200\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is an example of script to extract features from a map using our pipeline:"
      ],
      "metadata": {
        "id": "EQisl-D5okpC"
      },
      "id": "EQisl-D5okpC"
    },
    {
      "cell_type": "markdown",
      "id": "d2bdd4ba",
      "metadata": {
        "id": "d2bdd4ba"
      },
      "source": [
        "We start by selecting a map scan to layer it on the map by georeferencing. Then, extracting the labels from the legend to compare the colors of each segment. We save the extracted properties as a list into a csv."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing libraries\n",
        "from PIL import Image\n",
        "import folium\n",
        "from folium.map import Marker\n",
        "\n",
        "# Load map image\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "url = \"https://raw.githubusercontent.com/AnonymAuthors2025/Revised_Meta_Visualization/main/Stage1/soil.png\"\n",
        "response = requests.get(url)\n",
        "map_image = Image.open(BytesIO(response.content))  # Replace \"url/soil.png\" with your map image file"
      ],
      "metadata": {
        "id": "V32ctoxWowHX"
      },
      "id": "V32ctoxWowHX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Georeferencing\n",
        "soil = folium.Map(location=[33, 0], zoom_start=4)\n",
        "\n",
        "# Add a new image layer\n",
        "folium.raster_layers.ImageOverlay(\n",
        "    image='https://raw.githubusercontent.com/AnonymAuthors2025/Revised_Meta_Visualization/main/Stage1/soil.png',\n",
        "    bounds=[[9.9, -16.1], [39.1, 16.8]],\n",
        "    opacity=0.7,\n",
        "    name='My Image Layer'\n",
        ").add_to(soil)\n",
        "\n",
        "# Save the map and display it\n",
        "soil"
      ],
      "metadata": {
        "id": "M7RhhEGypYEC"
      },
      "id": "M7RhhEGypYEC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#label extraction + map segmentation\n",
        "from pyproj import Proj, transform\n",
        "import math\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from pyproj import Proj, transform\n",
        "import math\n",
        "\n",
        "def euclidean_distance(color1, color2):\n",
        "    r1, g1, b1 = color1\n",
        "    r2, g2, b2 = color2\n",
        "    return math.sqrt((r1 - r2) ** 2 + (g1 - g2) ** 2 + (b1 - b2) ** 2)\n",
        "\n",
        "def find_most_similar_color(rgb_color, color_list):\n",
        "    min_distance = float('inf')\n",
        "    most_similar_color = None\n",
        "\n",
        "    for color_name, color in color_list.items():\n",
        "        distance = euclidean_distance(rgb_color, color)\n",
        "        if distance < min_distance:\n",
        "            min_distance = distance\n",
        "            most_similar_color = (color, color_name)\n",
        "\n",
        "    return most_similar_color\n",
        "\n",
        "# Dictionary of colors with their names\n",
        "color_dict = {\n",
        "    'AR': (237, 232, 171),\n",
        "    'CL': (255, 255, 0),\n",
        "    'CM': (255, 166, 0),\n",
        "    'FL': (0, 191, 255),\n",
        "    'GY': (255, 250, 204),\n",
        "    'LP': (212, 212, 212),\n",
        "    'LV': (250, 128, 115),\n",
        "    'NT': (255, 161, 122),\n",
        "    'RG': (245, 222, 197),\n",
        "    'SC': (255, 0 , 255),\n",
        "    'VR': (148, 112, 219),\n",
        "    'WR': (135, 207, 250)\n",
        "}\n",
        "\n",
        "\n",
        "color_list = []\n",
        "\n",
        "# Load map image\n",
        "url = \"https://raw.githubusercontent.com/AnonymAuthors2025/Revised_Meta_Visualization/main/Stage1/soil.png\"\n",
        "response = requests.get(url)\n",
        "map_image = Image.open(BytesIO(response.content))  # Replace \"url/soil.png\" with your map image file\n",
        "\n",
        "# Define the bounding box of the map in terms of latitude and longitude\n",
        "# (min_latitude, min_longitude, max_latitude, max_longitude)\n",
        "map_bbox = (9.9, -16.1, 39.09, 16.79)  # Example bounding box bounds=[[9.9, -16.1], [39.1, 16.8]],\n",
        "\n",
        "# Determine the size of the map image in pixels\n",
        "map_width, map_height = map_image.size\n",
        "\n",
        "# Define the projection for the map (e.g., Web Mercator)\n",
        "map_projection = Proj(init='epsg:3857')\n",
        "\n",
        "# Convert latitude and longitude coordinates to pixel coordinates\n",
        "def lat_lon_to_pixel(lat, lon):\n",
        "    # Transform latitude and longitude to the map's projection\n",
        "    x, y = transform(Proj(init='epsg:4326'), map_projection, lon, lat)\n",
        "    # Scale the transformed coordinates to fit within the map image size\n",
        "    pixel_x = int((x - map_projection(map_bbox[1], map_bbox[0])[0]) /\n",
        "                  (map_projection(map_bbox[3], map_bbox[0])[0] - map_projection(map_bbox[1], map_bbox[0])[0]) * map_width)\n",
        "    pixel_y = int((map_projection(map_bbox[3], map_bbox[0])[1] - y) /\n",
        "                  (map_projection(map_bbox[3], map_bbox[2])[1] - map_projection(map_bbox[3], map_bbox[0])[1]) * map_height)\n",
        "    return pixel_x, pixel_y\n",
        "\n",
        "\n",
        "# Define intervals for latitude and longitude\n",
        "latitude_interval = (9.9, 39.1)  # Example latitude interval\n",
        "longitude_interval = (-16.1, 16.79)  # Example longitude interval\n",
        "\n",
        "# Define segment size for precision e.g., 0.1 for high precision, and 0.5 for medium precision ...\n",
        "latitude_step = 0.1\n",
        "longitude_step = 0.1\n",
        "\n",
        "\n",
        "# Generate latitude and longitude coordinates within the intervals\n",
        "locations = []\n",
        "for lat in range(int(latitude_interval[0] * 100), int(latitude_interval[1] * 100), int(latitude_step * 100)):\n",
        "    for lon in range(int(longitude_interval[0] * 100), int(longitude_interval[1] * 100), int(longitude_step * 100)):\n",
        "        locations.append((lat / 100, lon / 100))\n",
        "\n",
        "# Loop over each location and get the color\n",
        "for latitude, longitude in locations:\n",
        "    # Convert latitude and longitude to pixel coordinates\n",
        "    pixel_x, pixel_y = lat_lon_to_pixel(latitude, longitude)\n",
        "\n",
        "    # Get the color of the pixel at the specified location\n",
        "    pixel_color = map_image.getpixel((pixel_x, pixel_y))\n",
        "\n",
        "    # Find the most similar color from the dictionary\n",
        "    c, n = find_most_similar_color(pixel_color, color_dict)\n",
        "    color_list.append({\"latitude\": latitude, \"longitude\": longitude, \"color\": pixel_color, \"most similar\": c, \"type\": n})\n",
        "\n",
        "#print(color_list)"
      ],
      "metadata": {
        "id": "-i0Gw76drpFk"
      },
      "id": "-i0Gw76drpFk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d29fd55",
      "metadata": {
        "id": "7d29fd55"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(color_list)\n",
        "df.to_csv('https://raw.githubusercontent.com/AnonymAuthors2025/Revised_Meta_Visualization/main/Stage1/extracted_soil_types.csv', index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The output of this process is a list of (lat, long, property) sets covering the map, where each pair (lat, long) has its corresponding properties from the legend. Such list can be stored as a csv or any other format to feed different types of databases and GIS.**"
      ],
      "metadata": {
        "id": "AruweMT-sU-E"
      },
      "id": "AruweMT-sU-E"
    },
    {
      "cell_type": "markdown",
      "id": "785e6075",
      "metadata": {
        "id": "785e6075"
      },
      "source": [
        "Note that higher precision requires significanlty more time for extraction than medium and lower precision.\n",
        "The previous extraction step takes about 70 minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other tools exist for sophisticated pattern extraction, such as Rasterio/GDAL, QGIS (Manual validation) etc., but they are often costly and complex to handle by some user profiles.\n",
        "\n",
        "Here is an example of Rasterio extraction:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "import cv2, rasterio\n",
        "from geojson import FeatureCollection\n",
        "\n",
        "# Step 1: Preprocess scan\n",
        "image = cv2.imread(\"historical_map.png\")\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  \n",
        "\n",
        "# Step 2: Feature extraction (simplified)\n",
        "contours, _ = cv2.findContours(gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# Step 3: Georeference and export\n",
        "features = [contour_to_geojson(c) for c in contours]  # Custom function\n",
        "FeatureCollection(features).dump(\"output.geojson\")\n",
        "```\n"
      ],
      "metadata": {
        "id": "YJPNoXqmgLNT"
      },
      "id": "YJPNoXqmgLNT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "2. **Web scraping**\n",
        "\n",
        "This technique is becoming more and more adopted in data science projects where many libraries and APIs are being available for free. For example, Scrapy, BeautifulSoup, Selenium are some examples of Python libraries dedicated to web-scraping. Other API and tools exist such as Overpass Turbo, Google Map scraper etc.\n",
        "\n",
        "Here is an example of script for web-scraping from a website (FAO/AQUASTAT for weather data) using some Python libraries:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Constants\n",
        "BASE_URL = \"http://www.fao.org/aquastat/statistics/query/index.html\"\n",
        "USER_AGENT = \"ResearchBot (your-email@example.com)\"  # Identify yourself ethically\n",
        "DELAY = 2  # Seconds between requests\n",
        "\n",
        "def fetch_aquastat_data(country_code=\"EGY\", parameter=\"Precipitation\"):\n",
        "    \"\"\"\n",
        "    Scrapes AQUASTAT for country-specific weather data.\n",
        "    \n",
        "    Args:\n",
        "        country_code (str): 3-letter ISO country code (e.g., \"EGY\" for Egypt)\n",
        "        parameter (str): Target parameter (e.g., \"Precipitation\", \"Temperature\")\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: Extracted data table\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. Navigate to search page (POST request)\n",
        "        session = requests.Session()\n",
        "        headers = {\"User-Agent\": USER_AGENT}\n",
        "        \n",
        "        # 2. Submit search form (modify payload as needed)\n",
        "        payload = {\n",
        "            \"country\": country_code,\n",
        "            \"parameter\": parameter,\n",
        "            \"submit\": \"Submit\"\n",
        "        }\n",
        "        \n",
        "        response = session.post(BASE_URL, data=payload, headers=headers)\n",
        "        response.raise_for_status()  # Check for HTTP errors\n",
        "        \n",
        "        # 3. Parse HTML table\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        table = soup.find(\"table\", {\"class\": \"data\"})\n",
        "        \n",
        "        if not table:\n",
        "            raise ValueError(\"No data table found - check parameters or website structure\")\n",
        "        \n",
        "        # 4. Convert to DataFrame\n",
        "        rows = []\n",
        "        for tr in table.find_all(\"tr\")[1:]:  # Skip header\n",
        "            cells = [td.get_text(strip=True) for td in tr.find_all(\"td\")]\n",
        "            if cells:\n",
        "                rows.append(cells)\n",
        "        \n",
        "        df = pd.DataFrame(rows, columns=[\"Year\", parameter])\n",
        "        return df\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Fetch precipitation data for Egypt\n",
        "    df = fetch_aquastat_data(country_code=\"EGY\", parameter=\"Precipitation\")\n",
        "    if not df.empty:\n",
        "        print(df.head())\n",
        "        df.to_csv(\"aquastat_precipitation_egypt.csv\", index=False)\n",
        "    time.sleep(DELAY)  # Respect crawl delay\n",
        "```"
      ],
      "metadata": {
        "id": "VFwUb3b1mkeH"
      },
      "id": "VFwUb3b1mkeH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**The output of this script is a csv (tabular) file that can feed different types of databases and information systems.**\n",
        "\n",
        "\n",
        "**Ethical Considerations:**\n",
        "\n",
        "✔ Check privacy and intellectual property aspects before data extraction\n",
        "\n",
        "✔ Limit request rate (e.g., time.sleep())\n",
        "\n",
        "✔ Use APIs when available\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "+--------------+---------------------+---------+------------+\n",
        "| Library      | Use Case            | Speed   | Complexity |\n",
        "+--------------+---------------------+---------+------------+\n",
        "| BeautifulSoup| Static HTML         | Medium  | Low        |\n",
        "| Scrapy       | Large-scale scraping| High    | Medium     |\n",
        "| Selenium     | Dynamic content     | Low     | High       |\n",
        "+--------------+---------------------+---------+------------+\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7wz3pblEmnb3"
      },
      "id": "7wz3pblEmnb3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "3. **IoT devices:**\n",
        "\n",
        "IoT technology is becoming omnipresent in environmental monitoring, urban planning and many other projects related to spatiotemporal analytics. sensors, GPS trackers etc. are examples of devices used to generate data and feed databases and applications with required data on real-time. In soil sampling for instance, we use IoT devices to measure soil properties and send it to the Cloud.\n",
        "\n",
        "Common Protocols:\n",
        "\n",
        "1.   MQTT (Lightweight messaging)\n",
        "2.   REST APIs (Sensor gateways)\n",
        "3.   WebSocket (Real-time streams)\n",
        "\n",
        "\n",
        "Example: Sensor Data Pipeline\n",
        "\n",
        "\n",
        "```\n",
        "import paho.mqtt.client as mqtt\n",
        "\n",
        "def on_message(client, userdata, msg):\n",
        "    payload = json.loads(msg.payload)\n",
        "    save_to_database(payload)  # Custom function\n",
        "\n",
        "client = mqtt.Client()\n",
        "client.connect(\"iot_gateway.example.com\", 1883)\n",
        "client.subscribe(\"sensors/temperature\")\n",
        "```\n"
      ],
      "metadata": {
        "id": "W3A9MCiSqhCV"
      },
      "id": "W3A9MCiSqhCV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "## **Next Steps**\n",
        "\n",
        "- Run cells sequentially to generate sample datasets (csv).\n",
        "\n",
        "- Validate outputs with different stakeholders and domain-experts.\n",
        "\n",
        "- Proceed to Stage 2 (Data Integration)."
      ],
      "metadata": {
        "id": "Cpm_Rv1TvBgP"
      },
      "id": "Cpm_Rv1TvBgP"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
